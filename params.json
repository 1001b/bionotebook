{"name":"Spark Notebook","tagline":"Use Apache Spark straight from the Browser","body":"Spark Notebook\r\n==============\r\n\r\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/andypetrella/spark-notebook?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\r\n\r\n\r\n*Fork of the amazing [scala-notebook](https://github.com/Bridgewater/scala-notebook), yet focusing on Massive Dataset Analysis using [Apache Spark](http://spark.apache.org).*\r\n\r\n<!-- MarkdownTOC depth=4 autolink=true bracket=round -->\r\n\r\n- [Description](#description)\r\n  - [Discussions](#discussions)\r\n  - [Mailing list](#mailing-list)\r\n    - [Spark Notebook Dev](#spark-notebook-dev)\r\n    - [Spark Notebook User](#spark-notebook-user)\r\n- [Launch](#launch)\r\n  - [Using a release](#using-a-release)\r\n    - [Requirements](#requirements)\r\n    - [ZIP](#zip)\r\n    - [Docker](#docker)\r\n    - [boot2docker (Mac OS X)](#boot2docker-mac-os-x)\r\n    - [DEB](#deb)\r\n  - [From the sources](#from-the-sources)\r\n    - [Procedure](#procedure)\r\n- [Use](#use)\r\n- [Features](#features)\r\n  - [Use/Reconfigure Spark](#usereconfigure-spark)\r\n  - [Use the `form`](#use-the-form)\r\n  - [The `reset` function](#the-reset-function)\r\n  - [Keep an eye on your tasks](#keep-an-eye-on-your-tasks)\r\n  - [Using (Spark)SQL](#using-sparksql)\r\n    - [Static SQL](#static-sql)\r\n    - [Dynamic SQL](#dynamic-sql)\r\n    - [Show case](#show-case)\r\n  - [Interacting with JavaScript](#interacting-with-javascript)\r\n  - [Plotting with D3](#plotting-with-d3)\r\n  - [Timeseries with  Rickshaw](#timeseries-with--rickshaw)\r\n  - [Dynamic update of data and plot using Scala's `Future`](#dynamic-update-of-data-and-plot-using-scalas-future)\r\n  - [Update _Notebook_ `ClassPath`](#update-_notebook_-classpath)\r\n  - [Update __Spark__ dependencies (`spark.jars`)](#update-__spark__-dependencies-sparkjars)\r\n- [IMPORTANT](#important)\r\n- [KNOWN ISSUES](#known-issues)\r\n  - [`User limit of inotify watches reached`](#user-limit-of-inotify-watches-reached)\r\n\r\n<!-- /MarkdownTOC -->\r\n\r\nDescription\r\n-----------\r\nThe main intent of this tool is to create [reproducible analysis](http://simplystatistics.org/2014/06/06/the-real-reason-reproducible-research-is-important/) using Scala, Apache Spark and more.\r\n\r\nThis is achieved through an interactive web-based editor that can combine Scala code, SQL queries, Markup or even JavaScript in a collaborative manner.\r\n\r\nThe usage of Spark comes out of the box, and is simply enabled by the implicit variable named `sparkContext`.\r\n\r\n### Discussions\r\nC'mon on [gitter](https://gitter.im/andypetrella/spark-notebook?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)!\r\n\r\n### Mailing list\r\nThere are two different mailing lists, each aiming to specific discussions:\r\n\r\n#### Spark Notebook Dev\r\n\r\nThen [spark-notebook-dev](https://groups.google.com/forum/?hl=fr#!forum/spark-notebook-dev) mailing list for all threads regarding implementation, architecture, features and what not related to fix or enhance the project.\r\n\r\nEmail: [spark-notebook-dev@googlegroups.com](mailto:spark-notebook-dev@googlegroups.com).\r\n\r\n#### Spark Notebook User\r\n\r\nThe [spark-notebook-user](https://groups.google.com/forum/?hl=fr#!forum/spark-notebook-user) is for almost everything else than dev, which are questions, bugs, complains, or hopefully some kindness :-D.\r\n\r\nEmail: [spark-notebook-user@googlegroups.com](mailto:spark-notebook-user@googlegroups.com).\r\n\r\nLaunch\r\n------\r\n### Using a release\r\n\r\nLong story short, there are several ways to start the spark notebook quickly (even from scratch):\r\n * ZIP file\r\n * Docker image\r\n * DEB package\r\n\r\nHowever, there are several flavors for these distributions that depends on the Spark version and Hadoop version you are using.\r\n\r\n#### Requirements\r\n* Make sure you're running at least Java 7 (`sudo apt-get install openjdk-7-jdk`).\r\n\r\n#### ZIP\r\nThe zip distributions are publicly available in the bucket: <a href=\"http://s3.eu-central-1.amazonaws.com/spark-notebook/index.html\">s3://spark-notebook</a>.\r\n\r\n**Checkout** the needed version <a href=\"http://s3.eu-central-1.amazonaws.com/spark-notebook/index.html\">here</a>.\r\n\r\nHere is an example how to use it:\r\n```\r\nwget https://s3.eu-central-1.amazonaws.com/spark-notebook/zip/spark-notebook-0.2.0-spark-1.2.0-hadoop-1.0.4.zip\r\nunzip spark-notebook-0.2.0-spark-1.2.0-hadoop-1.0.4.zip\r\ncd spark-notebook-0.2.0-spark-1.2.0-hadoop-1.0.4\r\n./bin/spark-notebook\r\n```\r\n\r\n#### Docker\r\nIf you're a Docker user, the following procedure will be even simpler!\r\n\r\n**Checkout** the needed version <a href=\"https://registry.hub.docker.com/u/andypetrella/spark-notebook/tags/manage/\">here</a>.\r\n\r\n```\r\ndocker pull andypetrella/spark-notebook:0.2.0-spark-1.2.0-hadoop-1.0.4\r\ndocker run -p 9000:9000 andypetrella/spark-notebook:0.2.0-spark-1.2.0-hadoop-1.0.4\r\n```\r\n\r\n#### boot2docker (Mac OS X)\r\nOn Mac OS X, you need something like _boot2docker_ to use docker. However, port forwarding needs an extra command necessary for it to work (cf [this](http://stackoverflow.com/questions/28381903/spark-notebook-not-loading-with-docker) and [this](http://stackoverflow.com/questions/21653164/map-ports-so-you-can-access-docker-running-apps-from-osx-host) SO questions).\r\n\r\n```\r\nVBoxManage modifyvm \"boot2docker-vm\" --natpf1 \"tcp-port9000,tcp,,9000,,9000\"\r\n```\r\n\r\n\r\n#### DEB\r\nUsing debian packages is one of the standard, hence the spark notebook is also available in this form (from v0.2.0):\r\n\r\n\r\n```\r\nwget https://s3.eu-central-1.amazonaws.com/spark-notebook/deb/spark-notebook-0.2.0-spark-1.2.0-hadoop-1.0.4_all.deb\r\nsudo dpkg -i spark-notebook-0.2.0-spark-1.2.0-hadoop-1.0.4.zip\r\nsudo spark-notebook\r\n```\r\n\r\n**Checkout** the needed version <a href=\"http://s3.eu-central-1.amazonaws.com/spark-notebook/index.html\">here</a>.\r\n\r\n\r\n### From the sources\r\nThe spark notebook requires a [Java(TM)](http://en.wikipedia.org/wiki/Java_(programming_language)) environment (aka JVM) as runtime and [Play 2.2.6](https://www.playframework.com/documentation/2.2.6/Home) to build it.\r\n\r\nOf course, you will also need a working [GIT](http://git-scm.com/) installation to download the code and build it.\r\n\r\n#### Procedure\r\n##### Download the code\r\n```\r\ngit clone https://github.com/andypetrella/spark-notebook.git\r\ncd spark-notebook\r\n```\r\n##### Launch the server\r\nEnter the `play console` by running `play` within the `spark-notebook` folder:\r\n```\r\n[info] Loading global plugins from /home/noootsab/.sbt/0.13/plugins\r\n[info] Loading project definition from /home/Sources/noootsab/spark-notebook/project\r\n[info] Set current project to spark-notebook (in build file:/home/Sources/noootsab/spark-notebook/)\r\n       _\r\n _ __ | | __ _ _  _\r\n| '_ \\| |/ _' | || |\r\n|  __/|_|\\____|\\__ /\r\n|_|            |__/\r\n\r\nplay 2.2.6 built with Scala 2.10.3 (running Java 1.7.0_72), http://www.playframework.com\r\n\r\n> Type \"help play\" or \"license\" for more information.\r\n> Type \"exit\" or use Ctrl+D to leave this console.\r\n\r\n[spark-notebook] $\r\n```\r\n\r\nTo create your distribution\r\n```\r\n[spark-notebook] $ dist\r\n```\r\n\r\nIn order to develop on the Spark Notebook, you'll have to use the `run` command instead.\r\n\r\n\r\nUse\r\n---\r\nWhen the server has been started, you can head to the page `http://localhost:9000` and you'll see something similar to:\r\n![Notebook list](https://raw.github.com/andypetrella/spark-notebook/master/images/list.png)\r\n\r\nFrom there you can either:\r\n * create a new notebook or\r\n * launch an existing notebook\r\n\r\nIn both case, the `scala-notebook` will open a new tab with your notebook in it, loaded as a web page.\r\n\r\n> Note: a notebook is a JSON file containing the layout and analysis blocks, and it's located\r\n> within the project folder (with the `snb` extension).\r\n> Hence, they can be shared and we can track their history in an SVM like `GIT`.\r\n\r\nFeatures\r\n--------\r\n### Use/Reconfigure Spark\r\nSince this project aims directly the usage of Spark, a [SparkContext](https://github.com/apache/spark/blob/master/core%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2FSparkContext.scala) is added to the environment and can directly be used without additional effort.\r\n\r\n![Example using Spark](https://raw.github.com/andypetrella/spark-notebook/master/images/simplest-spark.png)\r\n\r\n\r\nSpark will start with a regular/basic configuration. There are different ways to customize the embedded Spark to your needs.\r\n\r\n### Use the `form`\r\nIn order to adapt the configuration of the `SparkContext`, one can add the widget `notebook.front.widgets.Spark`.\r\nThis widget takes the current context as only argument and will produce an HTML `form` that will allow manual and friendly changes to be applied.\r\n\r\nSo first, adding the widget in a cell,\r\n```{scala}\r\nimport notebook.front.widgets.Spark\r\nnew Spark(sparkContext)\r\n```\r\n\r\nRun the cell and you'll get,\r\n![Using SQL](https://raw.github.com/andypetrella/spark-notebook/master/images/update-spark-conf-form.png)\r\n\r\nIt has two parts:\r\n * the first one is showing an input for each current properties\r\n * the second will add new entries in the configuration based on the provided name\r\n\r\nSubmit the first part and the `SparkContext` will restart in the background (you can check the Spark UI to check if you like).\r\n\r\n\r\n### The `reset` function\r\nThe  *function* `reset` is available in all notebooks: This function takes several parameters, but the most important one is `lastChanges` which is itself a function that can adapt the [SparkConf](https://github.com/apache/spark/blob/master/core%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2FSparkConf.scala). This way, we can change the *master*, the *executor memory* and a *cassandra sink* or whatever before restarting it. For more Spark configuration options see: [Spark Configuration]([Spark Configuration](https://spark.apache.org/docs/1.1.0/configuration.html##available-properties))\r\n\r\nIn this example we reset `SparkContext` and add configuration options to use the [cassandra-connector]:\r\n```{scala}\r\nimport org.apache.spark.{Logging, SparkConf}\r\nval cassandraHost:String = \"localhost\"\r\nreset(lastChanges= _.set(\"spark.cassandra.connection.host\", cassandraHost))\r\n```\r\nThis makes Cassandra connector avaible in the Spark Context. Then you can use it, like so:\r\n```{scala}\r\nimport com.datastax.spark.connector._\r\nsparkContext.cassandraTable(\"test_keyspace\", \"test_column_family\")\r\n```\r\n\r\n### Keep an eye on your tasks\r\nAccessing the Spark UI is not always allowed or easy, hence a simple widget is available for us to keep a little eye on the stages running on the Spark cluster.\r\n\r\nLuckily, it's fairly easy, just add this to the notebook:\r\n```{scala}\r\nimport org.apache.spark.ui.notebook.front.widgets.SparkInfo\r\nimport scala.concurrent.duration._\r\nnew SparkInfo(sparkContext, checkInterval=1 second, execNumber=Some(100))\r\n```\r\nThis call will show and update a feedback panel tracking some basic (atm) metrics, in this configuration there will be **one check per second**, but will check only **100 times**.\r\n\r\nThis can be tuned at will, for instance for an infinte checking, one can pass the `None` value to the argument `execNumber`.\r\n\r\nCounting the words of a [wikipedia dump](http://en.wikipedia.org/wiki/Wikipedia:Database_download) will result in\r\n![Showing progress](https://raw.github.com/andypetrella/spark-notebook/master/images/spark-tracker.png)\r\n\r\n\r\n### Using (Spark)SQL\r\nSpark comes with this handy and cool feature that we can write some SQL queries rather than boilerplating with\r\nScala or whatever code, with the clear advantage that the resulting DAG is optimized.\r\n\r\nThe spark-notebook offers SparkSQL support.\r\nTo access it, we first we need to register an `RDD` as a table:\r\n```{scala}\r\ndataRDD.registerTempTable(\"data\")\r\n```\r\nNow, we can play with SQL in two different ways, the static and the dynamic ones.\r\n\r\n#### Static SQL\r\nThen we can play with this `data` table like so:\r\n```\r\n:sql select col1 from data where col2 == 'thingy'\r\n```\r\nThis will give access to the result via the `resXYZ` variable.\r\n\r\nThis is already helpful, but the `resXYZ` nummering can change and is not friendly, so we can also give a name to the result:\r\n```\r\n:sql[col1Var] select col1 from data where col2 == 'thingy'\r\n```\r\nNow, we can use the variable `col1Var` wrapping a `SchemaRDD`.\r\n\r\nThis variable is reactive meaning that it react to the change of the SQL result. Hence in order to deal with the result, you can access its `react` function which takes two arguments:\r\n * a **function** to apply on the underlying `SchemaRDD` to compute a result\r\n * a **widget** that will take the result of the function applied to the `SchemaRDD` and use it to update its rendering\r\n\r\nThe power of this reactivity is increased when we use SQL with dynamic parts.\r\n\r\n#### Dynamic SQL\r\nA dynamic SQL is looking like a static SQL but where specific tokens are used. Such tokens are taking the form: {`type`: `variableName`}.\r\n\r\nWhen executing the command, the notebook will produce a form by generating on input for each dynamic part. See the show case below.\r\n\r\nAn example of such dynamic SQL is\r\n```\r\n:sql[selectKids] SELECT name FROM people WHERE name = \"{String: name}\" and age >= {Int: age}\r\n```\r\nWhich will create a form with to inputs, one text and on number.\r\n\r\nWhen changing the value in the inputs, the SQL is compiled on the server and the result is printed on the notebook (Success, Failure, Bad Plan, etc.).\r\n\r\nAgain, the result is completely reactive, hence using the `react` function is mandatory to use the underlying SchemaRDD (when it becomes valid!).\r\n\r\n\r\n#### Show case\r\nThis is how it looks like in the notebook:\r\n\r\n![Using SQL](https://raw.github.com/andypetrella/spark-notebook/master/images/reactive-spark-sql.png)\r\n\r\n\r\n### Interacting with JavaScript\r\nShowing numbers can be good but great analysis reports should include relevant charts, for that we need JavaScript to manipulate the notebook's DOM.\r\n\r\nFor that purpose, a notebook can use the `Playground` abstraction. It allows us to create data in Scala and use it in predefined JavaScript functions (located under `assets/javascripts/notebook`) or even JavaScript snippets (that is, written straight in the notebook as a Scala `String` to be sent to the JavaScript interpreter).\r\n\r\nThe JavaScript function will be called with these parameters:\r\n * the data observable: a JS function can register its new data via `subscribe`.\r\n * the dom element: so that it can update it with custom behavior\r\n * an extra object: any additional data, configuration or whatever that comes from the Scala side\r\n\r\nHere is how this can be used, with a predefined `consoleDir` JS function ([see here](https://github.com/andypetrella/spark-notebook/blob/master/observable/src/main/assets/observable/js/consoleDir.coffee)):\r\n\r\n![Simple Playground](https://raw.github.com/andypetrella/spark-notebook/master/images/simple-pg.png)\r\n\r\nAnother example using the same predefined function and example to react on the new incoming data (more in further section). The __new stuff__ here is the use of `Codec` to convert a Scala object into the JSON format used in JS:\r\n\r\n![Playground with custom data](https://raw.github.com/andypetrella/spark-notebook/master/images/data-pg.png)\r\n\r\n\r\n### Plotting with [D3](http://d3js.org/)\r\nPlotting with D3.js is rather common now, however it's not always simple, hence there is a Scala wrapper that brings the bootstrap of D3 in the mix.\r\n\r\nThese wrappers are `D3.svg` and `D3.linePlot`, and they are just a proof of concept for now. The idea is to bring Scala data to D3.js then create `Coffeescript` to interact with them.\r\n\r\nFor instance, `linePlot` is used like so:\r\n\r\n![Using Rickshaw](https://raw.github.com/andypetrella/spark-notebook/master/images/linePlot.png)\r\n\r\n\r\n> Note: This is subject to future change because it would be better to use `playground` for this purpose.\r\n\r\n### Timeseries with  [Rickshaw](http://code.shutterstock.com/rickshaw/)\r\nPlotting timeseries is very common, for this purpose the spark notebook includes Rickshaw that quickly enables handsome timeline charts.\r\n\r\nRickshaw is available through `Playground` and a dedicated function for simple needs `rickshawts`.\r\n\r\nTo use it, you are only required to convert/wrap your data points into a dedicated `Series` object:\r\n```{scala}\r\ndef createTss(start:Long, step:Int=60*1000, nb:Int = 100):Seq[Series] = ...\r\nval data = createTss(orig, step, nb)\r\n\r\nval p = new Playground(data, List(Script(\"rickshawts\",\r\n                                          Json.obj(\r\n                                            \"renderer\" → \"stack\",\r\n                                            \"fixed\" → Json.obj(\r\n                                                        \"interval\" → (step/1000),\r\n                                                        \"max\" → 100,\r\n                                                        \"baseInSec\" → (orig/1000)\r\n                                                      )\r\n                                          ))))(seriesCodec)\r\n```\r\nAs you can see, the only big deal is to create the timeseries (`Seq[Series]` which is a simple wrapper around:\r\n * name\r\n * color\r\n * data (a sequence of `x` and `y`)\r\n\r\nAlso, there are some options to tune the display:\r\n * provide the type of renderer (`line`, `stack`, ...)\r\n * if the timeseries will be updated you can fix the window by supplying the `fixed` object:\r\n  * interval (at which data is upated)\r\n  * max (the max number of points displayed)\r\n  * the unit in the `X` axis.\r\n\r\nHere is an example of the kind of result you can expect:\r\n\r\n![Using Rickshaw](https://raw.github.com/andypetrella/spark-notebook/master/images/use-rickshaw.png)\r\n\r\n\r\n### Dynamic update of data and plot using Scala's `Future`\r\nOne of the very cool things that is used in the original `scala-notebook` is the use of reactive libs on both sides: server and client, combined with WebSockets. This offers a neat way to show dynamic activities like streaming data and so on.\r\n\r\nWe can exploit the reactive support to update Plot wrappers (the `Playground` instance actually) in a dynamic manner. If the JS functions are listening to the data changes they can automatically update their result.\r\n\r\nThe following example is showing how a timeseries plotted with Rickshaw can be regularly updated. We are using Scala `Futures` to simulate a server side process that would poll for a third-party service:\r\n\r\n![Update Timeseries Result](https://raw.github.com/andypetrella/spark-notebook/master/images/dyn-ts-code.png)\r\n\r\nThe results will be:\r\n\r\n![Update Timeseries Result](https://raw.github.com/andypetrella/spark-notebook/master/images/dyn-ts.gif)\r\n\r\n\r\n### Update _Notebook_ `ClassPath`\r\nKeeping your notebook runtime updated with the libraries you need in the classpath is usually cumbersome as it requires updating the server configuration in the SBT definition and restarting the system. Which is pretty sad because it requires a restart, rebuild and is not contextual to the notebook!\r\n\r\nHence, a dedicated context has been added to the block, `:cp` which allows us to add specifiy __local paths__ to jars that will be part of the classpath.\r\n\r\n```\r\n:cp /home/noootsab/.m2/repository/joda-time/joda-time/2.4/joda-time-2.4.jar\r\n```\r\nOr even\r\n```\r\n:cp\r\n/tmp/scala-notebook/repo/com/codahale/metrics/metrics-core/3.0.2/metrics-core-3.0.2.jar\r\n/tmp/scala-notebook/repo/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar\r\n/tmp/scala-notebook/repo/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar\r\n/tmp/scala-notebook/repo/joda-time/joda-time/2.3/joda-time-2.3.jar\r\n/tmp/scala-notebook/repo/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar\r\n/tmp/scala-notebook/repo/com/datastax/cassandra/cassandra-driver-core/2.0.4/cassandra-driver-core-2.0.4.jar\r\n/tmp/scala-notebook/repo/org/apache/thrift/libthrift/0.9.1/libthrift-0.9.1.jar\r\n/tmp/scala-notebook/repo/org/apache/httpcomponents/httpcore/4.2.4/httpcore-4.2.4.jar\r\n/tmp/scala-notebook/repo/org/joda/joda-convert/1.2/joda-convert-1.2.jar\r\n/tmp/scala-notebook/repo/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar\r\n/tmp/scala-notebook/repo/org/apache/cassandra/cassandra-clientutil/2.0.9/cassandra-clientutil-2.0.9.jar\r\n/tmp/scala-notebook/repo/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar\r\n/tmp/scala-notebook/repo/com/datastax/cassandra/cassandra-driver-core/2.0.4/cassandra-driver-core-2.0.4-sources.jar\r\n/tmp/scala-notebook/repo/io/netty/netty/3.9.0.Final/netty-3.9.0.Final.jar\r\n/tmp/scala-notebook/repo/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar\r\n/tmp/scala-notebook/repo/commons-codec/commons-codec/1.6/commons-codec-1.6.jar\r\n/tmp/scala-notebook/repo/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar\r\n/tmp/scala-notebook/repo/org/apache/cassandra/cassandra-thrift/2.0.9/cassandra-thrift-2.0.9.jar\r\n/tmp/scala-notebook/repo/com/datastax/spark/spark-cassandra-connector_2.10/1.1.0-alpha1/spark-cassandra-connector_2.10-1.1.0-alpha1.jar\r\n/tmp/scala-notebook/repo/com/google/guava/guava/15.0/guava-15.0.jar\r\n```\r\n\r\nHere is what it'll look like in the notebook:\r\n\r\n![Simple Classpath](https://raw.github.com/andypetrella/spark-notebook/master/images/simple-cp.png)\r\n\r\n### Update __Spark__ dependencies (`spark.jars`)\r\nSo you use Spark, hence you know that it's not enough to have the jars locally added to the Driver's classpath.\r\n\r\nIndeed, workers needs to have them in their classpath. One option would be to update the list of jars (`spark.jars` property) provided to the `SparkConf` using the `reset` function.\r\n\r\nHowever, this can be very tricky when we need to add jars that have themselves plenty of dependencies.\r\n\r\nHowever, there is another context available to update both the classpath on the notebook and in Spark\r\n\r\n```\r\n:dp\r\n+ group1 % artifact1 % version1\r\n+ group2 % artifact2 % version2\r\ngroup3 % artifact3 % version3\r\n+ group4 % artifact4 % version4\r\n\r\n- group5 % artifact5 % version5\r\n+ group6 % artifact6 % version6\r\n- group7 % artifact7 % version7\r\n```\r\n\r\nSo this is simple:\r\n* lines starting with `-` are exclusions (transitive)\r\n* lines starting with `+` or nothing are inclusions\r\n\r\nThe jars will be fetched in a temporary repository (that can be hardcoded using `:local-repo`).\r\n\r\nThen they'll be added to the Spark's jars property, before restarting the context.\r\n\r\n\r\nFor example, if you want to use [ADAM](https://github.com/bigdatagenomics/adam), all you need to do is:\r\n```\r\n:dp org.bdgenomics.adam % adam-apis % 0.15.0\r\n- org.apache.hadoop % hadoop-client %   _\r\n- org.apache.spark  %     _         %   _\r\n- org.scala-lang    %     _         %   _\r\n- org.scoverage     %     _         %   _\r\n```\r\n\r\nIn live, you can check the notebook named `Update classpath and Spark's jars`, which looks like this:\r\n\r\n![Spark Jars](https://raw.github.com/andypetrella/spark-notebook/master/images/spark-jars.png)\r\n\r\n## IMPORTANT\r\nSome vizualizations (wisp) are currently using Highcharts which is **not** available for commercial or private usage!\r\n\r\nIf you're in this case, please to <a href=\"email:andy.petrella@gmail.com\">contact</a> me first.\r\n\r\n\r\n## KNOWN ISSUES\r\n\r\n### `User limit of inotify watches reached`\r\n\r\nWhen running Spark-Notebook on some Linux distribs (specifically ArchLinux), you may encounter this exception:\r\n\r\n```\r\n[spark-notebook] $ run\r\n \r\njava.io.IOException: User limit of inotify watches reached\r\nat sun.nio.fs.LinuxWatchService$Poller.implRegister(LinuxWatchService.java:261)\r\nat sun.nio.fs.AbstractPoller.processRequests(AbstractPoller.java:260)\r\nat sun.nio.fs.LinuxWatchService$Poller.run(LinuxWatchService.java:326)\r\nat java.lang.Thread.run(Thread.java:745)\r\n[trace] Stack trace suppressed: run last sparkNotebook/compile:run for the full output.\r\n[error] (sparkNotebook/compile:run) java.lang.reflect.InvocationTargetException\r\n[error] Total time: 1 s, completed Jan 31, 2015 7:21:58 PM \r\n```\r\n\r\nThis certainly means your `sysctl` configuration limits too much `inotify` watches.\r\n\r\nYou must increase the parameter `fs.inotify.max_user_watches`.\r\n\r\nTo get current value:\r\n\r\n```\r\n$ sudo sysctl -a | grep fs.inotify.max_user_watches\r\nfs.inotify.max_user_watches = 8192\r\n```\r\n\r\nTo increase this value, create a new file `/etc/sysctl.d99-sysctl.conf`\r\n\r\n```\r\nfs.inotify.max_user_watches=100000\r\n```\r\n\r\nRefresh your live `sysctl` configuration:\r\n\r\n```\r\n$ sudo sysctl --system\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}